{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tab1","text":""},{"location":"#autoware-low-speed-autonomy-vehicles","title":"Autoware - Low Speed Autonomy Vehicles","text":"<p>The repo describes the goal, use case, and design of low speed autonomy vehicles using Autoware.</p> <p>To learn more about how to participate in this project, please read the onboarding guide</p>"},{"location":"#value-proposition","title":"Value Proposition","text":"<p>Low-speed autonomous (LSA) vehicles are envisioned as a practical near-term solution for specific use cases, particularly in controlled environments including university campuses, industrial sites, resorts, and warehouse. These vehicles, often operating at speeds below 25 mph, are seen as a stepping stone towards full autonomy and offer a more manageable environment for testing and deployment of autonomous technologies. They can address immediate needs like last-mile delivery, campus transportation, and on-site logistics, while also providing valuable data and experience for the development of higher-speed autonomous systems. </p> <p>According to a number of market research reports, the total projected market size of LSA vehicles in 2035 will be more than 113.96B USD in the United States. The following figure shows the market size of different use case categories.</p> Use Case Category Segment (if applicable) Current Market Value (USD Billion) Year of Current Value Projected Market Value (USD Billion) Year of Projection Compound Annual Growth Rate (CAGR) Source(s) Notes Cargo-Moving Utility Terrain Vehicles (UTVs) $6.642 2024 $10.48 2032 5.9% [1] This market size refers to the global Utility Terrain Vehicle (UTV) market as a whole, which is used to transport equipment, feed, and personnel [1]. Warehousing - Mixed Use Total Market ~$4.7 2024 ~$13.8 2030 ~20% [2] This total market includes Hybrid AMRs, Autonomous Yard Trucks, and Outdoor-Enabled Forklifts. The demand for end-to-end automation drives the fastest growth in the hybrid AMR segment [2]. Hybrid AMRs (Indoor/Outdoor) 2.5 2024 ~8 2030 ~22% [2] This segment is noted as the fastest growing due to demand for end-to-end automation [2]. Autonomous Yard Trucks 1.2 2024 ~3.5 2030 18% [2] These are seeing strong adoption in manufacturing and port-adjacent warehouses [2]. Outdoor-Enabled Forklifts 1 2024 ~2.3 2030 15% [2] Ports Autonomous Driving Solutions ~$1.5\u20132.0 2024 ~$6\u20138 2030 20\u201325% [3] This market is driven by efficiency demands, labor shortages, and sustainability goals, including Autonomous Terminal Trucks, AGVs, ASCs, and Software [3]. Airports Autonomous Driving Solutions $1.0\u20131.5 2024 $4.5\u20136.0 2030 20\u201325% [4] Driven by rising passenger traffic, labor shortages, and smart airport initiatives, this includes Baggage Tractors, Cargo Loaders, Passenger Shuttles, and Baggage Handling Robots [4]. Low-speed Logistics Autonomous Yard-Truck Solutions $0.8 2024 $2.2 2030 \u224822% [5] Demand for these solutions is strongest in fenced yards and mega-plants where 24/7 trailer or pallet moves create bottlenecks [5]. Outdoor-capable AMRs (heavy-payload) 2.3 2024 4.6 2030 \u224815% [5] Service Outdoor Cleaning Robots (all) $18 2024 $41 2030 18% [6] This is a broad category for outdoor cleaning robots, which may include both autonomous and non-autonomous solutions [6]. Autonomous Street Sweepers 0.45 2024 1.2 2033 12.5% [6] Market drivers include labor shortages, low-emission zones, and predictable, map-based routes [7]. Line-Marking Robots 0.08 2025 0.18 2033 15% [6] Campus Shuttle Service Shuttle Buses (overall market) $15.7 2023 ~$26.5 2032 &gt;6% (2024-2032) [8, 9] This market size refers to the overall shuttle bus market. The growth in this market from 2022-2032 is specifically driven by the increase in the market size of minibuses, which are typical for campus and micro-circulation shuttle services [8, 9]."},{"location":"#limitations-of-existing-technologies","title":"Limitations of Existing Technologies","text":"<p>Low speed vehicles drive a low speed and can reduce the injuries to the other road users. Many challenges remain open. </p>"},{"location":"#obstacle-perception-challenge","title":"Obstacle Perception Challenge","text":"<ul> <li>Lack of complete road infrastructure: </li> <li>Cost sensitive: </li> <li>Safety Priority:</li> <li>Mixed traffic flows: </li> </ul>"},{"location":"#driving-corridor-perception-challenge","title":"Driving Corridor Perception Challenge","text":""},{"location":"#safety-first","title":"Safety First","text":""},{"location":"#cost-reduction","title":"Cost reduction","text":"<p>The cost of LSA includes the capital cost and operation cost. </p>"},{"location":"#high-definition-maps","title":"High Definition Maps","text":"<p>(From PoV) We will not utilize 3D high definition prior maps, instead opting to use existing 2D navigational (sat-nav style) maps, also called ADAS maps. Human beings can drive on highways without prior knowledge of the detailed 3D geometry of roadways by relying on real-time scene perception, and our system aims to mimic this process.</p>"},{"location":"#end-to-end-ai-architecture","title":"End-to-End AI Architecture","text":"<p>(From PoV) We will follow an End-to-End AI Architecture in which each component technology of the highway pilot system is powered through neural-networks. We will follow a modular AI approach with component AI systems allowing for system explainability, introspection, verification and safety validation.</p>"},{"location":"#technology-roadmap","title":"Technology Roadmap","text":""},{"location":"#goal","title":"Goal","text":"<p>(From PoV) We will aim to develop true hands-off, eyes-off autonomous driving which is enabled on highways across the world, this will be classified as SAE Level-4 autonomy, where no human supervision is required of the vehicle.</p>"},{"location":"#technology-release-cycle","title":"Technology Release Cycle","text":"<p>(From PoV) We will iteratively and sequentially build the core software stack of the highway pilot system in a modular manner, and release versions of the autonomous highway pilot system that serve a core customer need, where each new release expands upon the operational-design-domain, technology prowess and safety envelope of the prior release.</p> <p> </p>"},{"location":"#vision-pilot","title":"Vision Pilot","text":"<p>(From PoV) Vision Pilot will enable SAE Level-3 autonomy over the full range of highway driving speeds (0 - 70mph). The system will be constrained to single lane driving (no lane changes, exits, on-ramps, roadworks), and will be operable on roads with clearly visible lanes. If these road conditions (ODD) are violated, then the system will perform a safe handover of control to the human driver, and if a handover is not possible, then the system will perform a Minimal Risk Manoeuvre to transition the self-driving car to as a safe state as is possible within its operational design domain. Vision Pilot's sensing suite will comprise two front-facing cameras, a main camera, as well as a long-range camera. Additionally, the system will integrate a front-facing 4D Imaging RADAR.</p>"},{"location":"#vision-pilot-plus","title":"Vision Pilot - Plus","text":"<p>Vision Pilot - Plus will enable SAE Level-3 autonomy over the full range of highway driving speeds (0 - 70mph), and will build on top of the functionality of Vision Pilot to enable fully autonomous lane changes - exits/ramps will be outside of the operational domain of the system. To enable autonomous lane changes, Vision Pilot Plus will additionally utilise 360 degree RADAR sensing through the introduction of corner RADARs for blindspot monitoring and a rear-facing 4D Imaging RADAR for safe lane changes at highway driving speeds</p>"},{"location":"#vision-pilot-pro","title":"Vision Pilot - PRO","text":"<p>Vision Pilot - PRO will enable SAE Level-3 autonomy over the full range of highway driving speeds (0 - 70mph) for an entire highway journey, including fully autonomous lane changes, exits, and on/off-ramps - being able to traverse multiple distinct connected highways without any human intervention or guidance. To enable autonomous driving of an entire highway journey, Vision Pilot - PRO will be integrated with 2D/ADAS maps to have more detailed road context and prior information about highway road layouts for navigational purposes. It will also additionally include side view cameras as well as a rear-facing camera, which will be needed for negotiating highway merges/exits. The 360 degree camera coverage will also help in online map generation using 2D/ADAS maps as prior guidance.</p>"},{"location":"#vision-drive","title":"Vision Drive","text":"<p>Vision Drive will enable SAE Level-4 autonomy over the full range of highway driving speeds (0 - 70mph) for an entire highway journey, including fully autonomous lane changes, exits, and on/off-ramps without any requirement for human supervision or manual control. In order to ensure system safety and robustness, Vision Drive will additionally integrate Short-Wave-Infrared as well as Long-Wave-Infrared front facing cameras which will allow the vehicle to see through fog, dust, haze, smoke and see clearly in pitch-black conditions. This hyper-spectral imaging will enable Vision Drive to match or exceed human safety performance of the driving task, enabling through SAE Level-4 autonomy.</p>"},{"location":"ONBOARDING/","title":"Contributor On-boarding Guide","text":"<p>Thank you for your interest in helping contribute to our work at Autoware Foundation to develop an open-source autonomous highway pilot system for privately owned vehicles. To get started, please follow the steps below:</p>"},{"location":"ONBOARDING/#how-to-get-started","title":"How to Get Started:","text":"<ul> <li> <p>Give this Github repo a 'star' and review the README file to familiarize yourself with the project goals</p> </li> <li> <p>Join our Discord, and in the 'privately owned vehicles' channel, please add a brief message to introduce yourself. Here is the link to join our Discord: https://discord.com/invite/Q94UsPvReQ</p> </li> <li> <p>And this is the link to the privately owned vehicles discord channel</p> </li> <li> <p>Catch up on our latest discussions and meetings. We add all the meeting minutes and meeting recordings on our Github discussions page. Here is a link to see our previous meetings.</p> </li> <li> <p>Join and participate in the Privately Owned Vehicle work group meetings to learn about our developments, get started in understanding the codebase, hear from maintainers and industry experts, and begin making code contributions.</p> </li> <li>We meet every week on Monday. There are two meeting slots which you can join, (Slot 1 is more suited for people joining from East Asia) (Slot 2 is more suited for people joining from Europe/USA). You can add the meeting invite to your calendar from these links: Slot 1 Meeting Link and Slot 2 Meeting Link - we cover the same agenda items in both slots so you only need to join one of them as best fits your schedule.</li> </ul>"},{"location":"ONBOARDING/#contact","title":"Contact","text":"<p>If you have any queries, you can contact Muhammad Zain Khawaja, Senior Tech Lead at Autoware Foundation and Lead of the Privately Owned Vehicle Work Group within Autoware Foundation at the following email address:  </p>"},{"location":"ONBOARDING/#we-look-forward-to-your-participation-in-this-exciting-open-source-project","title":"We look forward to your participation in this exciting open-source project!","text":"<p>To stay up to date on developments related to Autoware, please visit our website and follow us on our social media channels: - Autoware Foundation Website - LinkedIn Page  - YouTube Channel - Twitter/X Page</p>"},{"location":"PoV-index/","title":"PoV index","text":"[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Discord](https://img.shields.io/discord/953808765935816715?label=Autoware%20Discord)](https://discord.com/invite/Q94UsPvReQ) ![GitHub commit activity](https://img.shields.io/github/commit-activity/m/autowarefoundation/autoware.privately-owned-vehicles) ![GitHub Repo stars](https://img.shields.io/github/stars/autowarefoundation/autoware.privately-owned-vehicles)  ![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&amp;logo=pytorch&amp;logoColor=white) ![OpenCV](https://img.shields.io/badge/OpenCV-27338e?style=for-the-badge&amp;logo=OpenCV&amp;logoColor=whit) ![ROS](https://img.shields.io/badge/ROS-22314E?style=for-the-badge&amp;logo=ROS&amp;logoColor=whit) [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/the-autoware-foundation) [![YouTube](https://img.shields.io/badge/YouTube-FF0000?style=for-the-badge&amp;logo=youtube&amp;logoColor=white)](https://www.youtube.com/@autowarefoundation) [![Website](https://img.shields.io/badge/website-000000?style=for-the-badge&amp;logo=About.me&amp;logoColor=white)](https://autoware.org/)"},{"location":"PoV-index/#autoware-autonomous-highway-pilot","title":"Autoware - Autonomous Highway Pilot","text":"<p>The goal for this effort in Autoware is to build an open-source highway autonomy system that can power safe, SAE Level-4, autonomous driving around the world. To learn more about how to participate in this project, please read the onboarding guide</p> <p> </p>"},{"location":"PoV-index/#value-proposition","title":"Value Proposition","text":"<p>It is estimated that over 20 Trillion passenger miles are driven on highways around the world, each year. This equates to 280 Billion hours of driver time spent behind the wheel. If an autonomous driving technology could be developed to a sufficiently advanced level such that people did not need to manually drive on highways, and could do other activities with their time, then this would unlock a $1 Trillion/annum market opportunity, if we were to simply cost the time of a person at around $4/hour (conservative estimate).</p> <p> </p>"},{"location":"PoV-index/#limitations-of-existing-technologies","title":"Limitations of Existing Technologies","text":"<p>Current autonomous highway pilot systems face challenges in achieving the safety and performance requirements for true hands-off, eyes-off, self-driving, where the human driver no longer has to pay attention to the road or be engaged with the driving task. Specifically, there are two areas where autonomous driving technologies are currently lacking: obstacle perception for out of domain objects, and driving corridor perception in challenging scenarios.</p>"},{"location":"PoV-index/#obstacle-perception-challenge","title":"Obstacle Perception Challenge","text":"<p>Highway pilot systems typically rely upon vision (cameras), and RADAR as primary sensors.</p> <p>RADAR is a highly useful sensor in detecting moving obstacles at long range in all weather and lighting conditions, especially other moving vehicles. However, RADAR sensors suffer from frequent false-positive detections for static objects. This is often caused by ground-reflections, multi-path reflections, and sensor noise. This means that it is not possible to reliably use existing off-the-shelf automotive RADAR to reliably detect all obstacles in the driving scene as static objects cannot be distinguished from noise.</p> <p>To complement RADAR, highway pilot systems utilize vision technologies. These usually consist of AI-powered bounding-box detectors or semantic segmentation systems that attempt to classify objects by their type - e.g. cars, buses, vans, trucks, pedestrians, etc.</p> <p>Typically, the obstacle detection logic in highway pilot systems is as follows: - Moving object detected in RADAR - Treated as True Positive - Moving or static objected detected in vision - Treated as True Positive - Static object detected only in RADAR - Treated as Noise</p> <p>This leaves a blindspot in obstacle perception for long-tail edge case scenarios, where, if a static object is not detected by the vision system, then the detection is ignored by the autonomous vehicle. Such edge-case scenarios are caused by out-of-domain objects that occur rarely in driving scenes, such as fire trucks, police cars, ambulances, an animal crossing the road, etc. or strange presentations of known objects, e.g. a vehicle that has over-turned on the highway. There are many publicly documented examples of accidents that have occured with autonomous vehicles in exactly these types of driving scenarios. </p> <p> </p>"},{"location":"PoV-index/#obstacle-perception-solution","title":"Obstacle Perception Solution","text":"<p>To address this challenge, we aim to develop an obstacle perception system for self-driving cars that can reliably and robustly detect every obstacle in the driving scene, irrespective of what that object is, whether it is a static or moving object, across all weather and lighting conditions, even if the self-driving car has never seen such an object type or object presentation before. </p> <p>Many developers feel that the way to address this challenge is to utilize LIDAR - however, LIDAR sensors suffer from severe noise in rain, snow and fog, making them unusable in these weather conditions and unable to address the obstacle perception challenge. Therefore, we will utilize vision as the primary sensor, alongside RADAR - however, we will utilize vision to not only detect objects using AI, but we will also utilize vision to calculate true metric depth of the driving scene and develop a best-in-class VIDAR (vision-LIDAR) that works at long-range and can reliably measure depth in those weather conditions where LIDAR struggles. We will also utilize state-of-the-art 4D Imaging Radar to have RADAR sensing with greater resolution allowing stronger noise filtering, enabling us to more reliably detect static objects in RADAR alone.</p> <p>Our combined obstacle perception stack is comprised of a Vision Stack, a Feature Fusion Layer and a RADAR Stack:</p> <p>Vision Stack</p> <ul> <li>SceneSeg - Vision based AI-powered segmentation of all obstacles</li> <li>Scene3D - Vision based obstacle detection using depth estimation</li> <li>DomainSeg - Semantic segmentation of roadwork zones and construction objects</li> <li>EgoSpace - Semantic segmentation of roadwork zones and construction objects</li> </ul> <p>Feature Fusion</p> <ul> <li>Sentry - Sentry performs fusion of Vision Stack obstacle perception outputs with Imaging RADAR and standard automotive RADAR detections. RADAR detections are back-projected to the image domain and the semantic segmentation output masks form the Vision Stack are used as a type of 'filter' to create different versions of the Raw RADAR pointcloud, e.g. RADAR detections which overlap with foreground object labels from SceneSeg, or RADAR detections for ground points using road labels from EgoSpace, etc. These filtered RADAR pointcloud variations are transformed into a birds-eye-view image with multiple channels, wherein each channel corresponds to a separate filtered version of the raw RADAR pointcloud, enabling diverse feature representation at the input data stage for downstream RADAR neural network blocks.</li> </ul> <p>RADAR Stack</p> <p>Our RADAR stack processes the multi-channel BEV RADAR image in two branches called Safety Shield and Drive3D:</p> <ul> <li>Safety Shield - object occupancy detection of static and dynamic objects, ensuring the system reliably detects every object in the scene with the aim to never have an at-fault autonomous vehicle crash</li> <li>Drive3D - a robust 3D detection, classification, and tracking system to understand the scene context of commonly occuring foreground objects such as cars, buses, vans, trucks, etc.</li> </ul>"},{"location":"PoV-index/#driving-corridor-perception-challenge","title":"Driving Corridor Perception Challenge","text":"<p>Existing highway pilot systems can reliably detect the driving corridor through lane lines. Lane line perception can be performed through either AI-based methods or Computer Vision methods, achieving performance on-par with human drivers. However, highway pilot systems struggle to detect the driving corridor in safety-critical edge case scenarios, such as situations where lanes are no longer visible due to road maintenance issues, snow, etc. and scenarios where the driving corridor is highly adaptable, e.g. roadworks with traffic cones and road barriers. There are many publicly documented examples of accidents that have occured due to autonomous vehicles failing to perceive the correct driving corridor in these types of edge case scenarios.</p> <p> </p>"},{"location":"PoV-index/#driving-corridor-perception-solution","title":"Driving Corridor Perception Solution","text":"<p>To solve the driving corridor perception challenge, we will develop a universal driving path detection system called Path Finder, using two independent driving corridor perception technologies:</p> <p>Path Finder will be comprised of two branches: - EgoLanes - lane line and road edge detection - EgoPath - end-to-end prediction of driving corridors on roads with and without lanes</p> <p>By using an ensemble approach, Path Finder will be able to robustly tackle edge case driving scnearios and ensure autonomous vehicle safety.</p>"},{"location":"PoV-index/#high-definition-maps","title":"High Definition Maps","text":"<p>We will not utilize 3D high definition prior maps, instead opting to use existing 2D navigational (sat-nav style) maps, also called ADAS maps. Human beings can drive on highways without prior knowledge of the detailed 3D geometry of roadways by relying on real-time scene perception, and our system aims to mimic this process.</p>"},{"location":"PoV-index/#end-to-end-ai-architecture","title":"End-to-End AI Architecture","text":"<p>We will follow an End-to-End AI Architecture in which each component technology of the highway pilot system is powered through neural-networks. We will follow a modular AI approach with component AI systems allowing for system explainability, introspection, verification and safety validation.</p>"},{"location":"PoV-index/#vision-pipeline","title":"Vision Pipeline","text":"<p>The AutoSeg foundation model is currently being developed as part of the vision pipeline of the Autoware Highway Pilot System. It includes the development and implementation of SceneSeg, Scene3D, DomainSeg, EgoSpace, EgoPath and EgoLanes.</p>"},{"location":"PoV-index/#technology-roadmap","title":"Technology Roadmap","text":""},{"location":"PoV-index/#goal","title":"Goal","text":"<p>We will aim to develop true hands-off, eyes-off autonomous driving which is enabled on highways across the world, this will be classified as SAE Level-4 autonomy, where no human supervision is required of the vehicle.</p>"},{"location":"PoV-index/#technology-release-cycle","title":"Technology Release Cycle","text":"<p>We will iteratively and sequentially build the core software stack of the highway pilot system in a modular manner, and release versions of the autonomous highway pilot system that serve a core customer need, where each new release expands upon the operational-design-domain, technology prowess and safety envelope of the prior release.</p> <p>As part of our technology roadmap, we will sequentially develop four versions of the Autoware Autonomous Highway Pilot System, called: - Vision Pilot - Vision Pilot - Plus - Vision Pilot - PRO - Vision Drive</p> <p> </p>"},{"location":"PoV-index/#vision-pilot","title":"Vision Pilot","text":"<p>Vision Pilot will enable SAE Level-3 autonomy over the full range of highway driving speeds (0 - 70mph). The system will be constrained to single lane driving (no lane changes, exits, on-ramps, roadworks), and will be operable on roads with clearly visible lanes. If these road conditions (ODD) are violated, then the system will perform a safe handover of control to the human driver, and if a handover is not possible, then the system will perform a Minimal Risk Manoeuvre to transition the self-driving car to as a safe state as is possible within its operational design domain. Vision Pilot's sensing suite will comprise two front-facing cameras, a main camera, as well as a long-range camera. Additionally, the system will integrate a front-facing 4D Imaging RADAR.</p>"},{"location":"PoV-index/#vision-pilot-plus","title":"Vision Pilot - Plus","text":"<p>Vision Pilot - Plus will enable SAE Level-3 autonomy over the full range of highway driving speeds (0 - 70mph), and will build on top of the functionality of Vision Pilot to enable fully autonomous lane changes - exits/ramps will be outside of the operational domain of the system. To enable autonomous lane changes, Vision Pilot Plus will additionally utilise 360 degree RADAR sensing through the introduction of corner RADARs for blindspot monitoring and a rear-facing 4D Imaging RADAR for safe lane changes at highway driving speeds</p>"},{"location":"PoV-index/#vision-pilot-pro","title":"Vision Pilot - PRO","text":"<p>Vision Pilot - PRO will enable SAE Level-3 autonomy over the full range of highway driving speeds (0 - 70mph) for an entire highway journey, including fully autonomous lane changes, exits, and on/off-ramps - being able to traverse multiple distinct connected highways without any human intervention or guidance. To enable autonomous driving of an entire highway journey, Vision Pilot - PRO will be integrated with 2D/ADAS maps to have more detailed road context and prior information about highway road layouts for navigational purposes. It will also additionally include side view cameras as well as a rear-facing camera, which will be needed for negotiating highway merges/exits. The 360 degree camera coverage will also help in online map generation using 2D/ADAS maps as prior guidance.</p>"},{"location":"PoV-index/#vision-drive","title":"Vision Drive","text":"<p>Vision Drive will enable SAE Level-4 autonomy over the full range of highway driving speeds (0 - 70mph) for an entire highway journey, including fully autonomous lane changes, exits, and on/off-ramps without any requirement for human supervision or manual control. In order to ensure system safety and robustness, Vision Drive will additionally integrate Short-Wave-Infrared as well as Long-Wave-Infrared front facing cameras which will allow the vehicle to see through fog, dust, haze, smoke and see clearly in pitch-black conditions. This hyper-spectral imaging will enable Vision Drive to match or exceed human safety performance of the driving task, enabling through SAE Level-4 autonomy.</p>"},{"location":"RefDeg-index/","title":"Introduction","text":""},{"location":"RefDeg-index/#about-reference-design-guideline-for-lsa-vehicles","title":"About Reference Design Guideline for LSA Vehicles","text":"<p>This document serves a guideline to design and deploy a TRL-6 low speed autonomy vehicle based on Autoware. The readers can take this document as a starting point to select and configure the hardware and software components of the vehicles.</p>"},{"location":"RefDeg-index/#reference-design-guideline-for-lsa-vehicles-documentation-structure","title":"Reference Design Guideline for LSA Vehicles documentation structure","text":"<p>The reference design WG publishes the guidelines for Low Speed Autonomy (LSA) vehicles, using the following document structure shown below.</p> <p></p> <p>For more details about the reference design WG, its goals and details of the Autoware Foundation working groups that oversees the project, refer to the Reference Design WG wiki</p>"},{"location":"RefDeg-index/#getting-started","title":"Getting started","text":"<ul> <li>ODD</li> <li>Hardware Configuration</li> <li>Software Configuration</li> <li>Evaluation and Testing</li> </ul>"},{"location":"RefDeg-index/#other-example-design","title":"Other Example Design","text":"<ul> <li>RoboRacer (F1Tenth) describes the design and implementation of racing robots using Autoware.</li> <li>GoKart describes the design and implementation of EV GoKart using Autoware.</li> <li>KWT LSV describes the design and implementation of the LSV by KWT.</li> <li>System configuration describes the components that make up LSA vehicles in terms of the required hardware and software.</li> </ul>"}]}